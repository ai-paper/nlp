# Natural Language Processing Paper

1. **Distributed Representations of Sentences and Documents**
2. **GloVe: Global Vectors for Word Representation**
3. **Skip-Thought Vectors**
4. **Convolutional Neural Networks for Sentence Classification**
5. **Character-level Convolutional Networks for Text Classification**
6. **Hierarchical Attention Networks for Document Classification**
7. **Neural Relation Extraction with Selective Attention over Instances**
8. **End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF**
9. **Sequence to Sequence Learning with Neural Networks**
10. **Convolutional Sequence to Sequence Learning**
11. **Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation**
12. **Phrase-Based & Neural Unsupervised Machine Translation**
13. **Get To The Point: Summarization with Pointer-Generator Networks**
14. **End-To-End Memory Networks**
15. **QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension**
16. **Bidirectional Attention Flow for Machine Comprehension**
17. **Adversarial Learning for Neural Dialogue Generation**
18. **SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient**
19. **Modeling Relational Data with Graph Convolutional Networks**
20.  **Exploring the Limits of Language Modeling**
21. **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context**
22. **An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling**
23. **Deep contextualized word representations**
24. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**
25. **Efficient Estimation of Word Representations in Vector Space**
26. **Neural Machine Translation by Jointly Learning to Align and Translate**
27. **Attention is all you need**
28. **A Convolutional Neural Network for Modelling Sentences**
29. **fasttext：Bag of Tricks for Efficient Text Classification**
30. **Siamese recurrent architectures for learning sentence similarity**

